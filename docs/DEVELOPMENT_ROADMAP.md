# LXRT - Raport Rozwojowy dla Lidera Projektu
**Data:** 2026-01-21  
**Å¹rÃ³dÅ‚o:** Integracja z frameworkiem Stagehand (browser automation)  
**Autor:** AI Assistant (Antigravity)

---

## Podsumowanie Wykonawcze

Podczas integracji LXRT z Stagehand zidentyfikowano **12 konkretnych obszarÃ³w** wymagajÄ…cych poprawy. PoniÅ¼ej przedstawiono szczegÃ³Å‚owÄ… listÄ™ z uzasadnieniami i przykÅ‚adami z rzeczywistego kodu.

---

## PRIORYTET KRYTYCZNY ğŸ”´

### 1. Brak funkcji `countTokens()`

**Problem:**  
Nie ma moÅ¼liwoÅ›ci sprawdzenia ile tokenÃ³w zajmuje tekst przed wysÅ‚aniem do modelu. Prowadzi to do nieprzewidywalnych obciÄ™Ä‡ lub bÅ‚Ä™dÃ³w "context overflow".

**Uzasadnienie:**  
Podczas ekstrakcji treÅ›ci ze strony `stallman.org` otrzymaliÅ›my 11,894 znakÃ³w. Nie wiedzÄ…c ile to tokenÃ³w, musieliÅ›my arbitralnie obciÄ…Ä‡ do 1500 znakÃ³w, tracÄ…c potencjalnie waÅ¼ne dane.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: RÄ™czne obcinanie bez wiedzy o tokenach
const text = sectionContent.text.slice(0, 1500); // Arbitralna wartoÅ›Ä‡!

// RÄ™czne ostrzeÅ¼enie
if (sectionContent.charCount > 1500) {
    console.warn(`âš ï¸ WARNING: Input truncated from ${sectionContent.charCount} to 1500 chars`);
}
```

**Proponowane API:**
```typescript
const tokenCount = await provider.countTokens(text);
const contextWindow = provider.getContextWindow(); // np. 4096

if (tokenCount > contextWindow - 512) { // -512 na odpowiedÅº
    text = text.slice(0, estimateCharsForTokens(contextWindow - 512));
}
```

**Estymowany nakÅ‚ad:** 2-3 dni

---

### 2. Wolny inference na CPU/WASM

**Problem:**  
Generowanie 64 tokenÃ³w trwa ~17 sekund na CPU. Dla responsywnych aplikacji to za wolno.

**Uzasadnienie:**  
Nasz cel byÅ‚o <10s. Nawet po agresywnej optymalizacji (zmniejszenie modelu, tokenÃ³w, inputu) nie udaÅ‚o siÄ™ zejÅ›Ä‡ poniÅ¼ej 17s.

**Dane benchmarkowe:**
| Konfiguracja | Czas |
|--------------|------|
| Phi-3 (3.8B), 512 tokenÃ³w, 4000 znakÃ³w | ~5 minut |
| Qwen 0.5B, 128 tokenÃ³w, 1500 znakÃ³w | 31.32s |
| Qwen 0.5B, 64 tokeny, 500 znakÃ³w | **17.56s** |

**Jak to obeszliÅ›my:**
```typescript
// Zmniejszenie wszystkiego co moÅ¼liwe
{ maxTokens: 64 }  // zamiast 512
sectionContent.text.slice(0, 500)  // zamiast 4000
model: 'Xenova/Qwen1.5-0.5B-Chat'  // zamiast Phi-3
```

**Propozycja rozwiÄ…zania:**
1. **WebGPU backend** - 10-50x przyspieszenie (priorytet!)
2. **SIMD optimizations** dla WASM
3. **Speculative decoding** dla szybszego generowania

**Estymowany nakÅ‚ad:** 2-4 tygodnie

---

### 3. Brak `getContextWindow()`

**Problem:**  
Nie ma sposobu na programowe sprawdzenie rozmiaru okna kontekstowego modelu.

**Uzasadnienie:**  
KaÅ¼dy model ma inny limit (Qwen: 4096, Phi-3: 4096, Llama: 8192). Bez tej informacji nie moÅ¼na dynamicznie dostosowaÄ‡ inputu.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Hardkodowany limit
const MAX_INPUT_CHARS = 1500; // Zgadujemy Å¼e to bezpieczne
```

**Proponowane API:**
```typescript
const model = await provider.getModelInfo();
console.log(model.contextWindow); // 4096
console.log(model.maxTokens); // 2048
```

**Estymowany nakÅ‚ad:** 1-2 dni

---

## PRIORYTET WYSOKI ğŸŸ¡

### 4. Brak Abort/Cancel dla inference

**Problem:**  
Nie moÅ¼na przerwaÄ‡ trwajÄ…cego inference. UÅ¼ytkownik musi czekaÄ‡ nawet jeÅ›li chce anulowaÄ‡.

**Uzasadnienie:**  
W aplikacji Stagehand, jeÅ›li uÅ¼ytkownik zamknie przeglÄ…darkÄ™ w trakcie generowania, proces nadal dziaÅ‚a w tle.

**Jak to obeszliÅ›my:**
```typescript
// Brak obejÅ›cia - musieliÅ›my czekaÄ‡ lub zabiÄ‡ proces
process.exit(0); // Brutalne rozwiÄ…zanie
```

**Proponowane API:**
```typescript
const controller = new AbortController();
const response = await provider.chat(messages, { 
    signal: controller.signal 
});

// Timeout po 10s
setTimeout(() => controller.abort(), 10000);
```

**Estymowany nakÅ‚ad:** 3-5 dni

---

### 5. Brak JSON Mode

**Problem:**  
Nie ma gwarancji Å¼e model zwrÃ³ci poprawny JSON. Trzeba parsowaÄ‡ regex i obsÅ‚ugiwaÄ‡ bÅ‚Ä™dy.

**Uzasadnienie:**  
Stagehand wymaga strukturalnych odpowiedzi (schematy Zod). MaÅ‚e modele (0.5B) czÄ™sto generujÄ… niepoprawny JSON.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Regex parsing z fallbackiem
try {
    const jsonMatches = response.content.match(/\{[\s\S]*?\}/g);
    if (jsonMatches) {
        for (const match of jsonMatches) {
            try {
                const parsed = JSON.parse(match);
                if (parsed.talks && parsed.talks.length > 0) {
                    return parsed;
                }
            } catch (e) { /* ignore */ }
        }
    }
} catch (parseError) {
    console.log("âš ï¸ Could not parse as JSON");
}
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    responseFormat: { 
        type: "json_object",
        schema: z.object({ talks: z.array(...) }) // Zod schema
    }
});
// Gwarantowany poprawny JSON lub error
```

**Estymowany nakÅ‚ad:** 1 tydzieÅ„

---

### 6. Brak Function Calling

**Problem:**  
Nie ma natywnego wsparcia dla tool/function calling, ktÃ³re jest standardem w nowoczesnych LLM API.

**Uzasadnienie:**  
Stagehand uÅ¼ywa function calling do sterowania przeglÄ…darkÄ… (click, type, extract). MusieliÅ›my to emulowaÄ‡ w prompcie.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Instrukcje w prompcie zamiast narzÄ™dzi
const prompt = `Extract first event: ${text}
Format: Date - Location - Title`;
// Zamiast:
// tools: [{ name: "extract_event", parameters: {...} }]
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    tools: [{
        type: "function",
        function: {
            name: "extract_event",
            description: "Extracts event from text",
            parameters: {
                type: "object",
                properties: {
                    date: { type: "string" },
                    location: { type: "string" }
                }
            }
        }
    }]
});

if (response.tool_calls) {
    const call = response.tool_calls[0];
    console.log(call.function.arguments); // { date: "Jan 23", location: "Atlanta" }
}
```

**Estymowany nakÅ‚ad:** 2 tygodnie

---

## PRIORYTET ÅšREDNI ğŸŸ¢

### 7. Problemy z Path Aliases w Build

**Problem:**  
Po kompilacji TypeScript, aliasy (`@domain/*`) nie sÄ… rozwiÄ…zywane, powodujÄ…c bÅ‚Ä™dy importu.

**Uzasadnienie:**  
Przy pierwszym uruchomieniu przykÅ‚adu Stagehand otrzymaliÅ›my bÅ‚Ä…d:
```
Cannot find module '@domain/errors'
```

**Jak to naprawiliÅ›my:**
```json
// package.json - musieliÅ›my dodaÄ‡ tsc-alias
"scripts": {
    "build": "tsc && tsc-alias"  // Dodatkowy krok!
}
```

**Propozycja:**
- RozwaÅ¼yÄ‡ uÅ¼ycie `tsup` lub `esbuild` zamiast raw `tsc`
- Lub doÅ‚Ä…czyÄ‡ `tsc-alias` jako dependency i zautomatyzowaÄ‡

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 8. Konflikt wersji ONNX Runtime

**Problem:**  
LXRT wymaga `onnxruntime-node@1.23.0`, ale `@huggingface/transformers` wymaga `1.21.0`.

**Uzasadnienie:**  
Przy instalacji otrzymaliÅ›my ostrzeÅ¼enia o konflikcie, a pÃ³Åºniej bÅ‚Ä™dy Å‚adowania modelu:
```
Protobuf parsing failed
```

**Jak to naprawiliÅ›my:**
```bash
# UsuniÄ™cie nadmiarowej zaleÅ¼noÅ›ci
npm uninstall onnxruntime-node
# UÅ¼ycie wersji z @huggingface/transformers
```

**Propozycja:**
- UsunÄ…Ä‡ bezpoÅ›redniÄ… zaleÅ¼noÅ›Ä‡ `onnxruntime-node` z package.json
- PolegaÄ‡ na wersji dostarczanej przez `@huggingface/transformers`

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 9. Brak typÃ³w dla Event Payloads

**Problem:**  
Eventy `progress` i `ready` nie majÄ… wyeksportowanych typÃ³w TypeScript.

**Uzasadnienie:**  
IDE nie podpowiada dostÄ™pnych pÃ³l, co utrudnia development.

**Jak to obeszliÅ›my:**
```typescript
// Zgadywanie struktury na podstawie logÃ³w
provider.on('progress', (data) => {
    const percent = data.progress?.toFixed(1) ?? '?';  // Nie wiemy czy istnieje!
    const file = data.file ?? 'unknown';
    const status = data.status ?? 'loading';
    // ...
});
```

**Proponowane typy:**
```typescript
// EksportowaÄ‡ z biblioteki:
export interface ProgressEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
    file: string;
    progress: number; // 0-100
    loaded: number;   // bytes
    total: number;    // bytes
    status: 'downloading' | 'loading' | 'ready';
}

export interface ReadyEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
}
```

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 10. Dokumentacja Streaming API

**Problem:**  
Brak dokumentacji jak uÅ¼ywaÄ‡ `provider.stream()`.

**Uzasadnienie:**  
MusieliÅ›my szukaÄ‡ w kodzie ÅºrÃ³dÅ‚owym (`grep_search stream`) aby znaleÅºÄ‡ Å¼e ta funkcja istnieje.

**Jak to odkryliÅ›my:**
```bash
# Szukanie w ÅºrÃ³dÅ‚ach
grep -r "stream" src/app/AIProvider.ts
# ZnaleÅºliÅ›my: async *stream(
```

**Propozycja:**
DodaÄ‡ do dokumentacji:
```markdown
## Streaming Responses

```typescript
for await (const token of provider.stream(messages, options)) {
    process.stdout.write(token);
}
```
```

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 11. Brak przykÅ‚adÃ³w integracji

**Problem:**  
Brak oficjalnych przykÅ‚adÃ³w integracji z popularnymi frameworkami.

**Uzasadnienie:**  
MusieliÅ›my od zera pisaÄ‡ `LxrtLLMProvider` dla Stagehand, zgadujÄ…c jak mapowaÄ‡ API.

**Jak to zrobiliÅ›my:**
```typescript
// NapisaliÅ›my wÅ‚asny adapter (109 linii kodu)
export class LxrtLLMProvider implements LLMClient {
    async createChatCompletion(options) {
        // Mapowanie Stagehand -> LXRT
        const messages = options.messages.map(m => ({
            role: m.role,
            content: typeof m.content === 'string' ? m.content : '...'
        }));
        // ...
    }
}
```

**Propozycja:**
StworzyÄ‡ oficjalne adaptery:
- `@lxrt/stagehand` - adapter dla Stagehand
- `@lxrt/langchain` - adapter dla LangChain.js
- `@lxrt/vercel-ai` - adapter dla Vercel AI SDK

**Estymowany nakÅ‚ad:** 1 tydzieÅ„ per adapter

---

### 12. Brak CLI do zarzÄ…dzania modelami

**Problem:**  
Nie ma sposobu na pre-download modeli przed uruchomieniem aplikacji.

**Uzasadnienie:**  
Pierwszy start aplikacji trwa dÅ‚ugo (pobieranie modelu). W produkcji chcemy mieÄ‡ modele juÅ¼ pobrane.

**Jak to obeszliÅ›my:**
```typescript
// Model pobiera siÄ™ przy pierwszym warmup()
await provider.warmup('llm'); // Tutaj dopiero Å›ciÄ…ga ~1GB
```

**Proponowane CLI:**
```bash
# Pre-download modeli
npx lxrt pull Xenova/Qwen1.5-0.5B-Chat --dtype q4

# Lista pobranych modeli
npx lxrt list

# UsuÅ„ model z cache
npx lxrt remove Xenova/Phi-3-mini-4k-instruct
```

**Estymowany nakÅ‚ad:** 1 tydzieÅ„

---

### 13. Brak Registry i Type-Safety dla modeli

**Problem:**  
Wszystkie konfiguracje modeli (`LLMConfig`, `STTConfig`) uÅ¼ywajÄ… typu `string` dla pola `model`. Brak weryfikacji czy model istnieje oraz brak autouzupeÅ‚niania w IDE.

**Uzasadnienie:**  
Programista musi znaÄ‡ dokÅ‚adne ID modelu z Hugging Face (np. `Xenova/whisper-tiny`). LiterÃ³wka powoduje bÅ‚Ä…d dopiero w runtime (przy prÃ³bie pobrania).

**Jak to obeszliÅ›my:**  
RÄ™czne wpisywanie stringÃ³w bez walidacji.

**Proponowane rozwiÄ…zanie (Model Registry):**
Implementacja podejÅ›cia "Registry + Type-Safety":
- **Registry:** Centralny plik `src/core/ModelRegistry.ts` z definicjami przetestowanych modeli.
- **Typy:** `type SupportedLLM = keyof typeof MODEL_REGISTRY.llm`.
- **Hybrid types:** `model: SupportedLLM | (string & {})` - zapewnia autouzupeÅ‚nianie dla znanych modeli, zachowujÄ…c moÅ¼liwoÅ›Ä‡ wpisania dowolnego stringa.

**Estymowany nakÅ‚ad:** 2-3 dni

---

## Podsumowanie PriorytetÃ³w

| # | Zadanie | Priorytet | NakÅ‚ad | WpÅ‚yw |
|---|---------|-----------|--------|-------|
| 1 | `countTokens()` | ğŸ”´ Krytyczny | 2-3 dni | Wysoki |
| 2 | WebGPU backend | ğŸ”´ Krytyczny | 2-4 tyg | Bardzo wysoki |
| 3 | `getContextWindow()` | ğŸ”´ Krytyczny | 1-2 dni | Wysoki |
| 4 | Abort/Cancel | ğŸŸ¡ Wysoki | 3-5 dni | Åšredni |
| 5 | JSON Mode | ğŸŸ¡ Wysoki | 1 tydzieÅ„ | Wysoki |
| 6 | Function Calling | ğŸŸ¡ Wysoki | 2 tygodnie | Wysoki |
| 7 | Fix path aliases | ğŸŸ¢ Åšredni | 0.5 dnia | Niski |
| 8 | Fix ONNX conflict | ğŸŸ¢ Åšredni | 0.5 dnia | Niski |
| 9 | Typy eventÃ³w | ğŸŸ¢ Åšredni | 0.5 dnia | Niski |
| 10 | Docs streaming | ğŸŸ¢ Åšredni | 0.5 dnia | Niski |
| 11 | Adaptery integracji | ğŸŸ¢ Åšredni | 3 tygodnie | Åšredni |
| 12 | CLI zarzÄ…dzania | ğŸŸ¢ Åšredni | 1 tydzieÅ„ | Åšredni |
| 13 | Model Registry & Types | ğŸŸ¢ Åšredni | 2-3 dni | Åšredni |

**Sugerowana kolejnoÅ›Ä‡ na nastÄ™pny cykl:**
1. Fix ONNX conflict + path aliases (szybkie wygrane)
2. `countTokens()` + `getContextWindow()` (krytyczne dla UX)
3. Abort/Cancel + typy eventÃ³w
4. Dokumentacja streaming + przykÅ‚ady
5. WebGPU (dÅ‚ugoterminowy, ale game-changer)

---

## PrzyszÅ‚e Rozszerzenia - Auto-Tuning System

### Implementacja w Fazach (patrz: autotuning_plan.md)

#### Faza 0: Model Presets âœ… ZAKOÅƒCZONE
**Status:** Implementacja statycznych presetÃ³w (`chat-light`, `embedding-quality`)  
**Cel:** Foundation dla auto-tuningu - semantic naming dla modeli  
**NakÅ‚ad:** 1-2 dni

#### Faza 1-5: Auto-Tuning (Priorytetowe)

| # | Funkcja | Priorytet | NakÅ‚ad | Opis |
|---|---------|-----------|--------|------|
| 1 | **Model Selection** | ğŸ”´ Bardzo wysoki | âœ… ZAKOÅƒCZONE | Auto-wybÃ³r modelu na podstawie RAM, GPU, platform |
| 2 | **DType Selection** | ğŸ”´ Wysoki | âœ… ZAKOÅƒCZONE | Auto kwantyzacja (fp16/q8/q4) na podstawie zasobÃ³w |
| 3 | **Performance Mode** | ğŸŸ¡ Åšredni | âœ… ZAKOÅƒCZONE | Auto fast/balanced/quality w zaleÅ¼noÅ›ci od Å›rodowiska |
| 4 | **WASM Threads** | âœ… JuÅ¼ dziaÅ‚a | âœ… ZAKOÅƒCZONE | Ulepszenia istniejÄ…cej logiki thread count |
| 5 | **Context/Tokens Limits** | ğŸŸ¢ Niski | âœ… ZAKOÅƒCZONE | Auto-limitowanie dla sÅ‚abych systemÃ³w (OOM prevention) |

**Total Faza 1-5:** ~8-12 dni roboczych

#### PrzyszÅ‚e Ulepszenia (PÃ³Åºniej)

| # | Funkcja | Priorytet | NakÅ‚ad | Opis |
|---|---------|-----------|--------|------|
| 6 | **Batch Size Tuning** | ğŸŸ¡ Åšredni | 2-3 dni | Automatyczny batch size dla embeddings na podstawie RAM/GPU |
| 7 | **Cache Strategy** | ğŸŸ¢ Niski | 3-5 dni | Inteligentne zarzÄ…dzanie cache (eviction, quota management) |
| 8 | **Inference Params** | ğŸŸ¢ Niski | 1-2 dni | Auto-tuning temperature, topK, topP dla rÃ³Å¼nych use-cases |

**PrzykÅ‚adowe API po auto-tuningu:**
```typescript
const provider = createAIProvider({
  llm: {
    preset: 'chat',      // intencja uÅ¼ytkownika
    autoTune: true       // auto: model + dtype + performance
  }
});

// System automatycznie wybiera:
// - Model: chat-light/medium/heavy na podstawie RAM & GPU  
// - DType: fp16/q8/q4 na podstawie capabilities
// - Performance: fast/balanced/quality
// - Threads: optimal count
// - MaxTokens: safe limits
```

**WiÄ™cej:** SzczegÃ³Å‚y implementacji w `autotuning_plan.md` (artifact)

---

## ZaÅ‚Ä…czniki

### A. Kod adaptera Stagehand
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/LxrtLLMProvider.ts`

### B. Zoptymalizowany przykÅ‚ad
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/index.ts`

### C. Benchmark wynikÃ³w
- Model: Qwen1.5-0.5B-Chat (q4)
- Czas: 17.56s dla 64 tokenÃ³w
- Platform: CPU/WASM (Linux x64)
