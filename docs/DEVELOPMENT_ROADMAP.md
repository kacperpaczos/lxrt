# LXRT - Raport Rozwojowy dla Lidera Projektu
**Data:** 2026-01-21  
**Å¹rÃ³dÅ‚o:** Integracja z frameworkiem Stagehand (browser automation)  
**Autor:** AI Assistant (Antigravity)

---

## Podsumowanie Wykonawcze

Podczas integracji LXRT z Stagehand zidentyfikowano **12 konkretnych obszarÃ³w** wymagajÄ…cych poprawy. PoniÅ¼ej przedstawiono szczegÃ³Å‚owÄ… listÄ™ z uzasadnieniami i przykÅ‚adami z rzeczywistego kodu.

---

## PRIORYTET KRYTYCZNY ðŸ”´

### 1. Brak funkcji `countTokens()`

**Problem:**  
Nie ma moÅ¼liwoÅ›ci sprawdzenia ile tokenÃ³w zajmuje tekst przed wysÅ‚aniem do modelu. Prowadzi to do nieprzewidywalnych obciÄ™Ä‡ lub bÅ‚Ä™dÃ³w "context overflow".

**Uzasadnienie:**  
Podczas ekstrakcji treÅ›ci ze strony `stallman.org` otrzymaliÅ›my 11,894 znakÃ³w. Nie wiedzÄ…c ile to tokenÃ³w, musieliÅ›my arbitralnie obciÄ…Ä‡ do 1500 znakÃ³w, tracÄ…c potencjalnie waÅ¼ne dane.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: RÄ™czne obcinanie bez wiedzy o tokenach
const text = sectionContent.text.slice(0, 1500); // Arbitralna wartoÅ›Ä‡!

// RÄ™czne ostrzeÅ¼enie
if (sectionContent.charCount > 1500) {
    console.warn(`âš ï¸ WARNING: Input truncated from ${sectionContent.charCount} to 1500 chars`);
}
```

**Proponowane API:**
```typescript
const tokenCount = await provider.countTokens(text);
const contextWindow = provider.getContextWindow(); // np. 4096

if (tokenCount > contextWindow - 512) { // -512 na odpowiedÅº
    text = text.slice(0, estimateCharsForTokens(contextWindow - 512));
}
```

**Estymowany nakÅ‚ad:** 2-3 dni

---

### 2. Wolny inference na CPU/WASM

**Problem:**  
Generowanie 64 tokenÃ³w trwa ~17 sekund na CPU. Dla responsywnych aplikacji to za wolno.

**Uzasadnienie:**  
Nasz cel byÅ‚o <10s. Nawet po agresywnej optymalizacji (zmniejszenie modelu, tokenÃ³w, inputu) nie udaÅ‚o siÄ™ zejÅ›Ä‡ poniÅ¼ej 17s.

**Dane benchmarkowe:**
| Konfiguracja | Czas |
|--------------|------|
| Phi-3 (3.8B), 512 tokenÃ³w, 4000 znakÃ³w | ~5 minut |
| Qwen 0.5B, 128 tokenÃ³w, 1500 znakÃ³w | 31.32s |
| Qwen 0.5B, 64 tokeny, 500 znakÃ³w | **17.56s** |

**Jak to obeszliÅ›my:**
```typescript
// Zmniejszenie wszystkiego co moÅ¼liwe
{ maxTokens: 64 }  // zamiast 512
sectionContent.text.slice(0, 500)  // zamiast 4000
model: 'Xenova/Qwen1.5-0.5B-Chat'  // zamiast Phi-3
```

**Propozycja rozwiÄ…zania:**
1. **WebGPU backend** - 10-50x przyspieszenie (priorytet!)
2. **SIMD optimizations** dla WASM
3. **Speculative decoding** dla szybszego generowania

**Estymowany nakÅ‚ad:** 2-4 tygodnie

---

### 3. Brak `getContextWindow()`

**Problem:**  
Nie ma sposobu na programowe sprawdzenie rozmiaru okna kontekstowego modelu.

**Uzasadnienie:**  
KaÅ¼dy model ma inny limit (Qwen: 4096, Phi-3: 4096, Llama: 8192). Bez tej informacji nie moÅ¼na dynamicznie dostosowaÄ‡ inputu.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Hardkodowany limit
const MAX_INPUT_CHARS = 1500; // Zgadujemy Å¼e to bezpieczne
```

**Proponowane API:**
```typescript
const model = await provider.getModelInfo();
console.log(model.contextWindow); // 4096
console.log(model.maxTokens); // 2048
```

**Estymowany nakÅ‚ad:** 1-2 dni

---

## PRIORYTET WYSOKI ðŸŸ¡

### 4. Brak Abort/Cancel dla inference

**Problem:**  
Nie moÅ¼na przerwaÄ‡ trwajÄ…cego inference. UÅ¼ytkownik musi czekaÄ‡ nawet jeÅ›li chce anulowaÄ‡.

**Uzasadnienie:**  
W aplikacji Stagehand, jeÅ›li uÅ¼ytkownik zamknie przeglÄ…darkÄ™ w trakcie generowania, proces nadal dziaÅ‚a w tle.

**Jak to obeszliÅ›my:**
```typescript
// Brak obejÅ›cia - musieliÅ›my czekaÄ‡ lub zabiÄ‡ proces
process.exit(0); // Brutalne rozwiÄ…zanie
```

**Proponowane API:**
```typescript
const controller = new AbortController();
const response = await provider.chat(messages, { 
    signal: controller.signal 
});

// Timeout po 10s
setTimeout(() => controller.abort(), 10000);
```

**Estymowany nakÅ‚ad:** 3-5 dni

---

### 5. Brak JSON Mode

**Problem:**  
Nie ma gwarancji Å¼e model zwrÃ³ci poprawny JSON. Trzeba parsowaÄ‡ regex i obsÅ‚ugiwaÄ‡ bÅ‚Ä™dy.

**Uzasadnienie:**  
Stagehand wymaga strukturalnych odpowiedzi (schematy Zod). MaÅ‚e modele (0.5B) czÄ™sto generujÄ… niepoprawny JSON.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Regex parsing z fallbackiem
try {
    const jsonMatches = response.content.match(/\{[\s\S]*?\}/g);
    if (jsonMatches) {
        for (const match of jsonMatches) {
            try {
                const parsed = JSON.parse(match);
                if (parsed.talks && parsed.talks.length > 0) {
                    return parsed;
                }
            } catch (e) { /* ignore */ }
        }
    }
} catch (parseError) {
    console.log("âš ï¸ Could not parse as JSON");
}
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    responseFormat: { 
        type: "json_object",
        schema: z.object({ talks: z.array(...) }) // Zod schema
    }
});
// Gwarantowany poprawny JSON lub error
```

**Estymowany nakÅ‚ad:** 1 tydzieÅ„

---

### 6. Brak Function Calling

**Problem:**  
Nie ma natywnego wsparcia dla tool/function calling, ktÃ³re jest standardem w nowoczesnych LLM API.

**Uzasadnienie:**  
Stagehand uÅ¼ywa function calling do sterowania przeglÄ…darkÄ… (click, type, extract). MusieliÅ›my to emulowaÄ‡ w prompcie.

**Jak to obeszliÅ›my:**
```typescript
// WORKAROUND: Instrukcje w prompcie zamiast narzÄ™dzi
const prompt = `Extract first event: ${text}
Format: Date - Location - Title`;
// Zamiast:
// tools: [{ name: "extract_event", parameters: {...} }]
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    tools: [{
        type: "function",
        function: {
            name: "extract_event",
            description: "Extracts event from text",
            parameters: {
                type: "object",
                properties: {
                    date: { type: "string" },
                    location: { type: "string" }
                }
            }
        }
    }]
});

if (response.tool_calls) {
    const call = response.tool_calls[0];
    console.log(call.function.arguments); // { date: "Jan 23", location: "Atlanta" }
}
```

**Estymowany nakÅ‚ad:** 2 tygodnie

---

## PRIORYTET ÅšREDNI ðŸŸ¢

### 7. Problemy z Path Aliases w Build

**Problem:**  
Po kompilacji TypeScript, aliasy (`@domain/*`) nie sÄ… rozwiÄ…zywane, powodujÄ…c bÅ‚Ä™dy importu.

**Uzasadnienie:**  
Przy pierwszym uruchomieniu przykÅ‚adu Stagehand otrzymaliÅ›my bÅ‚Ä…d:
```
Cannot find module '@domain/errors'
```

**Jak to naprawiliÅ›my:**
```json
// package.json - musieliÅ›my dodaÄ‡ tsc-alias
"scripts": {
    "build": "tsc && tsc-alias"  // Dodatkowy krok!
}
```

**Propozycja:**
- RozwaÅ¼yÄ‡ uÅ¼ycie `tsup` lub `esbuild` zamiast raw `tsc`
- Lub doÅ‚Ä…czyÄ‡ `tsc-alias` jako dependency i zautomatyzowaÄ‡

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 8. Konflikt wersji ONNX Runtime

**Problem:**  
LXRT wymaga `onnxruntime-node@1.23.0`, ale `@huggingface/transformers` wymaga `1.21.0`.

**Uzasadnienie:**  
Przy instalacji otrzymaliÅ›my ostrzeÅ¼enia o konflikcie, a pÃ³Åºniej bÅ‚Ä™dy Å‚adowania modelu:
```
Protobuf parsing failed
```

**Jak to naprawiliÅ›my:**
```bash
# UsuniÄ™cie nadmiarowej zaleÅ¼noÅ›ci
npm uninstall onnxruntime-node
# UÅ¼ycie wersji z @huggingface/transformers
```

**Propozycja:**
- UsunÄ…Ä‡ bezpoÅ›redniÄ… zaleÅ¼noÅ›Ä‡ `onnxruntime-node` z package.json
- PolegaÄ‡ na wersji dostarczanej przez `@huggingface/transformers`

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 9. Brak typÃ³w dla Event Payloads

**Problem:**  
Eventy `progress` i `ready` nie majÄ… wyeksportowanych typÃ³w TypeScript.

**Uzasadnienie:**  
IDE nie podpowiada dostÄ™pnych pÃ³l, co utrudnia development.

**Jak to obeszliÅ›my:**
```typescript
// Zgadywanie struktury na podstawie logÃ³w
provider.on('progress', (data) => {
    const percent = data.progress?.toFixed(1) ?? '?';  // Nie wiemy czy istnieje!
    const file = data.file ?? 'unknown';
    const status = data.status ?? 'loading';
    // ...
});
```

**Proponowane typy:**
```typescript
// EksportowaÄ‡ z biblioteki:
export interface ProgressEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
    file: string;
    progress: number; // 0-100
    loaded: number;   // bytes
    total: number;    // bytes
    status: 'downloading' | 'loading' | 'ready';
}

export interface ReadyEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
}
```

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 10. Dokumentacja Streaming API

**Problem:**  
Brak dokumentacji jak uÅ¼ywaÄ‡ `provider.stream()`.

**Uzasadnienie:**  
MusieliÅ›my szukaÄ‡ w kodzie ÅºrÃ³dÅ‚owym (`grep_search stream`) aby znaleÅºÄ‡ Å¼e ta funkcja istnieje.

**Jak to odkryliÅ›my:**
```bash
# Szukanie w ÅºrÃ³dÅ‚ach
grep -r "stream" src/app/AIProvider.ts
# ZnaleÅºliÅ›my: async *stream(
```

**Propozycja:**
DodaÄ‡ do dokumentacji:
```markdown
## Streaming Responses

```typescript
for await (const token of provider.stream(messages, options)) {
    process.stdout.write(token);
}
```
```

**Estymowany nakÅ‚ad:** 0.5 dnia

---

### 11. Brak przykÅ‚adÃ³w integracji

**Problem:**  
Brak oficjalnych przykÅ‚adÃ³w integracji z popularnymi frameworkami.

**Uzasadnienie:**  
MusieliÅ›my od zera pisaÄ‡ `LxrtLLMProvider` dla Stagehand, zgadujÄ…c jak mapowaÄ‡ API.

**Jak to zrobiliÅ›my:**
```typescript
// NapisaliÅ›my wÅ‚asny adapter (109 linii kodu)
export class LxrtLLMProvider implements LLMClient {
    async createChatCompletion(options) {
        // Mapowanie Stagehand -> LXRT
        const messages = options.messages.map(m => ({
            role: m.role,
            content: typeof m.content === 'string' ? m.content : '...'
        }));
        // ...
    }
}
```

**Propozycja:**
StworzyÄ‡ oficjalne adaptery:
- `@lxrt/stagehand` - adapter dla Stagehand
- `@lxrt/langchain` - adapter dla LangChain.js
- `@lxrt/vercel-ai` - adapter dla Vercel AI SDK

**Estymowany nakÅ‚ad:** 1 tydzieÅ„ per adapter

---

### 12. Brak CLI do zarzÄ…dzania modelami

**Problem:**  
Nie ma sposobu na pre-download modeli przed uruchomieniem aplikacji.

**Uzasadnienie:**  
Pierwszy start aplikacji trwa dÅ‚ugo (pobieranie modelu). W produkcji chcemy mieÄ‡ modele juÅ¼ pobrane.

**Jak to obeszliÅ›my:**
```typescript
// Model pobiera siÄ™ przy pierwszym warmup()
await provider.warmup('llm'); // Tutaj dopiero Å›ciÄ…ga ~1GB
```

**Proponowane CLI:**
```bash
# Pre-download modeli
npx lxrt pull Xenova/Qwen1.5-0.5B-Chat --dtype q4

# Lista pobranych modeli
npx lxrt list

# UsuÅ„ model z cache
npx lxrt remove Xenova/Phi-3-mini-4k-instruct
```

**Estymowany nakÅ‚ad:** 1 tydzieÅ„

---

### 13. Brak Registry i Type-Safety dla modeli

**Problem:**  
Wszystkie konfiguracje modeli (`LLMConfig`, `STTConfig`) uÅ¼ywajÄ… typu `string` dla pola `model`. Brak weryfikacji czy model istnieje oraz brak autouzupeÅ‚niania w IDE.

**Uzasadnienie:**  
Programista musi znaÄ‡ dokÅ‚adne ID modelu z Hugging Face (np. `Xenova/whisper-tiny`). LiterÃ³wka powoduje bÅ‚Ä…d dopiero w runtime (przy prÃ³bie pobrania).

**Jak to obeszliÅ›my:**  
RÄ™czne wpisywanie stringÃ³w bez walidacji.

**Proponowane rozwiÄ…zanie (Model Registry):**
Implementacja podejÅ›cia "Registry + Type-Safety":
- **Registry:** Centralny plik `src/core/ModelRegistry.ts` z definicjami przetestowanych modeli.
- **Typy:** `type SupportedLLM = keyof typeof MODEL_REGISTRY.llm`.
- **Hybrid types:** `model: SupportedLLM | (string & {})` - zapewnia autouzupeÅ‚nianie dla znanych modeli, zachowujÄ…c moÅ¼liwoÅ›Ä‡ wpisania dowolnego stringa.

**Estymowany nakÅ‚ad:** 2-3 dni

---

### 14. Robust Integration Testing (Prawdziwe modele + Determinizm)

**Problem:**
Testy integracyjne (np. STT -> LLM) sÄ… "flaky" (niestabilne) z powodu niedoskonaÅ‚oÅ›ci maÅ‚ych modeli (Whisper Tiny) na syntetycznych danych lub szumie. Workaroundy (jak `if text == '!!!'`) sÄ… tymczasowe.

**RozwiÄ…zanie (Jak):**
1.  **Golden Datasets:** Stworzenie repozytorium prawdziwych prÃ³bek audio (human voice, clear speech) zamiast generowanych/pustych.
2.  **Semantic Assertions:** Weryfikacja poprawnoÅ›ci nie przez `text.length > 0`, ale przez podobieÅ„stwo semantyczne (np. czy odpowiedÅº LLM ma sens w kontekÅ›cie).
3.  **Determinizm:** Ustawienie `seed` dla modeli (jeÅ›li wspierane) oraz `temperature=0` w testach.

**Estymowany nakÅ‚ad:** 2-3 dni

---

## Podsumowanie PriorytetÃ³w

| # | Zadanie | Priorytet | NakÅ‚ad | WpÅ‚yw |
|---|---------|-----------|--------|-------|
| 1 | `countTokens()` | ðŸ”´ Krytyczny | âœ… DONE | Wysoki |
| 2 | WebGPU backend | ðŸ”´ Krytyczny | 2-4 tyg | Bardzo wysoki |
| 3 | `getContextWindow()` | ðŸ”´ Krytyczny | âœ… DONE | Wysoki |
| 4 | Abort/Cancel | ðŸŸ¡ Wysoki | 3-5 dni | Åšredni |
| 5 | JSON Mode | ðŸŸ¡ Wysoki | 1 tydzieÅ„ | Wysoki |
| 6 | Function Calling | ðŸŸ¡ Wysoki | 2 tygodnie | Wysoki |
| 7 | Fix path aliases | ðŸŸ¢ Åšredni | âœ… DONE | Niski |
| 8 | Fix ONNX conflict | ðŸŸ¢ Åšredni | âœ… DONE | Niski |
| 9 | Typy eventÃ³w | ðŸŸ¢ Åšredni | âœ… DONE | Niski |
| 10 | Docs streaming | ðŸŸ¢ Åšredni | âœ… DONE | Niski |
| 11 | Adaptery integracji | ðŸŸ¢ Åšredni | 3 tygodnie | Åšredni |
| 12 | CLI zarzÄ…dzania | ðŸŸ¢ Åšredni | 1 tydzieÅ„ | Åšredni |
| 13 | Model Registry & Types | ðŸŸ¢ Åšredni | âœ… DONE | Åšredni |
| 14 | **Robust Integration Testing** | ðŸŸ¢ Åšredni | âœ… DONE | Åšredni |
| 15 | Refactor `Error` to `ModelNotLoadedError` | ðŸŸ¢ Niski | âœ… DONE | Niski |
| 16 | Unify error strings (constants) | ðŸŸ¢ Niski | âœ… DONE | Niski |
| 17 | **Test Quality Review & Rewrite** | ðŸŸ¢ Åšredni | âœ… DONE | Åšredni |
| 18 | **Stagehand Interface** | ðŸŸ¢ Åšredni | âœ… DONE | Wysoki |

**Sugerowana kolejnoÅ›Ä‡ na nastÄ™pny cykl:**
1. Fix ONNX conflict + path aliases (szybkie wygrane)
2. `countTokens()` + `getContextWindow()` (krytyczne dla UX)
3. **Robust Integration Testing** (blokuje CI/CD)
4. **Stagehand Interface** (WaÅ¼ne dla integracji)
5. **Test Quality Review** (DÅ‚ug techniczny)
6. Abort/Cancel + typy eventÃ³w
5. Dokumentacja streaming + przykÅ‚ady
6. WebGPU (dÅ‚ugoterminowy, ale game-changer)

---

## PrzyszÅ‚e Rozszerzenia - Auto-Tuning System

### Implementacja w Fazach (patrz: autotuning_plan.md)

#### Faza 0: Model Presets âœ… ZAKOÅƒCZONE
**Status:** Implementacja statycznych presetÃ³w (`chat-light`, `embedding-quality`)  
**Cel:** Foundation dla auto-tuningu - semantic naming dla modeli  
**NakÅ‚ad:** 1-2 dni

#### Faza 1-5: Auto-Tuning (Priorytetowe)

| # | Funkcja | Priorytet | NakÅ‚ad | Opis |
|---|---------|-----------|--------|------|
| 1 | **Model Selection** | ðŸ”´ Bardzo wysoki | âœ… ZAKOÅƒCZONE | Auto-wybÃ³r modelu na podstawie RAM, GPU, platform |
| 2 | **DType Selection** | ðŸ”´ Wysoki | âœ… ZAKOÅƒCZONE | Auto kwantyzacja (fp16/q8/q4) na podstawie zasobÃ³w |
| 3 | **Performance Mode** | ðŸŸ¡ Åšredni | âœ… ZAKOÅƒCZONE | Auto fast/balanced/quality w zaleÅ¼noÅ›ci od Å›rodowiska |
| 4 | **WASM Threads** | âœ… JuÅ¼ dziaÅ‚a | âœ… ZAKOÅƒCZONE | Ulepszenia istniejÄ…cej logiki thread count |
| 5 | **Context/Tokens Limits** | ðŸŸ¢ Niski | âœ… ZAKOÅƒCZONE | Auto-limitowanie dla sÅ‚abych systemÃ³w (OOM prevention) |

**Total Faza 1-5:** ~8-12 dni roboczych


### Specyfikacja Refaktoryzacji (Zadania #15, #16)

#### Zadanie #15: Refactor `Error` to `ModelNotLoadedError`

**Cel:** UmoÅ¼liwienie programistycznej obsÅ‚ugi bÅ‚Ä™dÃ³w (np. auto-warmup po zÅ‚apaniu bÅ‚Ä™du).

**Wymagania:**
1.  StworzyÄ‡ klasÄ™ `ModelNotLoadedError` w `src/domain/errors.ts`:
    ```typescript
    export class ModelNotLoadedError extends BaseError {
      constructor(
        message: string,
        public modality: Modality,
        public modelId?: string
      ) {
        super(message);
        this.name = 'ModelNotLoadedError';
      }
    }
    ```
2.  **Use Cases (Gdzie uÅ¼yÄ‡):**
    *   `AIProvider.countTokens()` - gdy config istnieje, ale model nie loaded.
    *   `AIProvider.getContextWindow()`
    *   `AIProvider.chat()`, `speak()`, `listen()` - zastÄ…piÄ‡ obecne `ValidationError` lub generic `Error` tam, gdzie sprawdzany jest stan zaÅ‚adowania.

#### Zadanie #16: Unify Error Strings

**Cel:** UnikniÄ™cie literÃ³wek i niespÃ³jnych komunikatÃ³w ("Model not loaded" vs "Load model first").

**Wymagania:**
1.  UtworzyÄ‡ `src/core/error-messages.ts`:
    ```typescript
    export const ERRORS = {
      MODEL: {
        NOT_LOADED: (modality: string) => 
          `Model for ${modality} must be loaded. Call warmup('${modality}') first.`,
        NOT_CONFIGURED: (modality: string) =>
          `${modality} not configured. Add config to createAIProvider().`,
      },
      // ...
    } as const;
    ```
2.  ZastÄ…piÄ‡ hardcoded stringi w `AIProvider.ts` i `LLMModel.ts`.

---

#### PrzyszÅ‚e Ulepszenia (PÃ³Åºniej)

| # | Funkcja | Priorytet | NakÅ‚ad | Opis |
|---|---------|-----------|--------|------|
| 6 | **Batch Size Tuning** | ðŸŸ¡ Åšredni | 2-3 dni | Automatyczny batch size dla embeddings na podstawie RAM/GPU |
| 7 | **Cache Strategy** | ðŸŸ¢ Niski | 3-5 dni | Inteligentne zarzÄ…dzanie cache (eviction, quota management) |
| 8 | **Inference Params** | ðŸŸ¢ Niski | 1-2 dni | Auto-tuning temperature, topK, topP dla rÃ³Å¼nych use-cases |

**PrzykÅ‚adowe API po auto-tuningu:**
```typescript
const provider = createAIProvider({
  llm: {
    preset: 'chat',      // intencja uÅ¼ytkownika
    autoTune: true       // auto: model + dtype + performance
  }
});

// System automatycznie wybiera:
// - Model: chat-light/medium/heavy na podstawie RAM & GPU  
// - DType: fp16/q8/q4 na podstawie capabilities
// - Performance: fast/balanced/quality
// - Threads: optimal count
// - MaxTokens: safe limits
```

**WiÄ™cej:** SzczegÃ³Å‚y implementacji w `autotuning_plan.md` (artifact)

---

## Audyt Techniczny â€” Plan DziaÅ‚ania (2026-01-28)

### Metryki WyjÅ›ciowe
- **Source LoC:** 13,388 | **Test LoC:** 4,176 | **Test Ratio:** 0.31 (target â‰¥0.5)
- **Explicit `any` w production:** 6 lokalizacji
- **TODOs w krytycznych Å›cieÅ¼kach:** 4
- **Debug console.log w production:** 150
- **Spin-lock polling patterns:** 7 lokalizacji

---

### ðŸ”´ P0 â€” Krytyczne (ZAKOÅƒCZONE âœ…)

- [x] **RozszerzyÄ‡ `ILLMModel` o brakujÄ…ce metody**
  - **Plik:** `src/domain/models/index.ts` L22-32
  - **Co:** Dodano `countTokens(text: string): number` i `getContextWindow(): number`
  - **Status:** âœ… DONE â€” UsuniÄ™to `(model as any)` bypass w `AIProvider.ts`

- [x] **ZastÄ…piÄ‡ spin-lock polling Promise chaining**
  - **Pliki:** `LLMModel.ts`, `STTModel.ts`, `TTSModel.ts`, `OCRModel.ts`, `EmbeddingModel.ts`, `BaseModel.ts`
  - **Co:** Zamieniono `while (this.loading) { await setTimeout(100) }` na `loadingPromise` pattern
  - **Status:** âœ… DONE â€” Dodano rÃ³wnieÅ¼ `loadingPromises` Map w `ModelManager` dla peÅ‚nej synchronizacji

---

### ðŸŸ¡ P1 â€” Wysokie (ZAKOÅƒCZONE âœ…)

- [x] **DodaÄ‡ `AbortSignal` support do inference**
  - **Pliki:** `src/core/types.ts`, `src/models/LLMModel.ts`
  - **Co:** Dodano `signal?: AbortSignal` do `ChatOptions` i `CompletionOptions`
  - **Status:** âœ… DONE â€” Przekazuje `abort_signal` do Transformers.js pipeline

- [ ] **StworzyÄ‡ GitHub Actions CI workflow**
  - **Plik:** `.github/workflows/ci.yml` (nowy)
  - **Co:** Build + lint + test:unit + npm audit na kaÅ¼dy PR
  - **Effort:** 1 dzieÅ„

- [ ] **UsunÄ…Ä‡/zastÄ…piÄ‡ debug console.log Loggerem**
  - **Zakres:** 150 statements w `src/` (gÅ‚Ã³wnie `src/models/`)
  - **Co:** UsunÄ…Ä‡ lub przekierowaÄ‡ do `Logger` interface
  - **Effort:** 1 dzieÅ„

---

### ðŸŸ¢ P2 â€” Åšrednie (CZÄ˜ÅšCIOWO ZAKOÅƒCZONE)

- [x] **NaprawiÄ‡ conditional import JSDOM**
  - **Plik:** `src/app/vectorization/VectorizationService.ts`
  - **Status:** âœ… DONE â€” UÅ¼yto dynamic `await import('jsdom')`

- [x] **DodaÄ‡ testy jednostkowe dla modeli**
  - **Pliki:** `tests/node/unit/stt-model.test.ts`, `tts-model.test.ts`, `ocr-model.test.ts`
  - **Status:** âœ… DONE

- [x] **DodaÄ‡ testy integracyjne dla concurrency**
  - **Pliki:** `tests/node/integration/concurrent-load.test.ts`, `abort-signal.test.ts`
  - **Status:** âœ… DONE

- [ ] **ZaimplementowaÄ‡ TODOs w VectorizationService**
  - **Lokalizacje:** L725, L740
  - **Effort:** 3-5 dni

- [ ] **DodaÄ‡ job cancellation do React/Vue hooks**
  - **Pliki:** `src/ui/react/useVectorization.ts`, `src/ui/vue/useVectorization.ts`
  - **Effort:** 4h

---

### ðŸ”µ P3 â€” Niskie / Rekomendacje

- [ ] **Weryfikacja cache modeli (Model Persistence Test)**
  - **Cel:** UpewniÄ‡ siÄ™, Å¼e LXRT pamiÄ™ta zaÅ‚adowany model i nie pobiera go za kaÅ¼dym razem
  - **Co:** DodaÄ‡ integration test sprawdzajÄ…cy Å¼e 2x warmup() nie powoduje 2x download
  - **Effort:** 2h

- [ ] **ZaprojektowaÄ‡ szynÄ™ logÃ³w (Logging Bus)**
  - **Cel:** Centralny system logowania dostÄ™pny dla developerÃ³w i testÃ³w
  - **Wymagania:**
    - Interface `LogBus` z metodami `log()`, `warn()`, `error()`, `debug()`
    - MoÅ¼liwoÅ›Ä‡ subskrypcji logÃ³w w testach (`logBus.subscribe()`)
    - Integracja z istniejÄ…cym `Logger` z `domain/logging/`
  - **Effort:** 1-2 dni

- [ ] **WprowadziÄ‡ enum ErrorPattern dla caÅ‚ej aplikacji**
  - **Cel:** Type-safe error patterns zamiast string matching
  - **Wymagania:**
    ```typescript
    export enum ErrorPattern {
      MODEL_NOT_LOADED = 'MODEL_NOT_LOADED',
      MODEL_LOAD_FAILED = 'MODEL_LOAD_FAILED',
      INFERENCE_ABORTED = 'INFERENCE_ABORTED',
      VALIDATION_FAILED = 'VALIDATION_FAILED',
      // ...
    }
    ```
  - **Gdzie uÅ¼yÄ‡:** `src/domain/errors.ts`, wszystkie klasy bÅ‚Ä™dÃ³w
  - **Effort:** 0.5 dnia

- [ ] **DodaÄ‡ `implements IModel` do BaseModel**
  - **Plik:** `src/models/BaseModel.ts` L8
  - **Effort:** 30min

- [ ] **UsunÄ…Ä‡ pozostaÅ‚e `any` w StagehandAdapter**
  - **Plik:** `src/adapters/StagehandAdapter.ts` L26, L57
  - **Effort:** 1h

---

## ZaÅ‚Ä…czniki

### A. Kod adaptera Stagehand
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/LxrtLLMProvider.ts`

### B. Zoptymalizowany przykÅ‚ad
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/index.ts`

### C. Benchmark wynikÃ³w
- Model: Qwen1.5-0.5B-Chat (q4)
- Czas: 17.56s dla 64 tokenÃ³w
- Platform: CPU/WASM (Linux x64)
