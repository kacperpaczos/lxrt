# LXRT - Raport Rozwojowy dla Lidera Projektu
**Data:** 2026-01-21  
**≈πr√≥d≈Ço:** Integracja z frameworkiem Stagehand (browser automation)  
**Autor:** AI Assistant (Antigravity)

---

## Podsumowanie Wykonawcze

Podczas integracji LXRT z Stagehand zidentyfikowano **12 konkretnych obszar√≥w** wymagajƒÖcych poprawy. Poni≈ºej przedstawiono szczeg√≥≈ÇowƒÖ listƒô z uzasadnieniami i przyk≈Çadami z rzeczywistego kodu.

---

## PRIORYTET KRYTYCZNY üî¥

### 1. Brak funkcji `countTokens()`

**Problem:**  
Nie ma mo≈ºliwo≈õci sprawdzenia ile token√≥w zajmuje tekst przed wys≈Çaniem do modelu. Prowadzi to do nieprzewidywalnych obciƒôƒá lub b≈Çƒôd√≥w "context overflow".

**Uzasadnienie:**  
Podczas ekstrakcji tre≈õci ze strony `stallman.org` otrzymali≈õmy 11,894 znak√≥w. Nie wiedzƒÖc ile to token√≥w, musieli≈õmy arbitralnie obciƒÖƒá do 1500 znak√≥w, tracƒÖc potencjalnie wa≈ºne dane.

**Jak to obeszli≈õmy:**
```typescript
// WORKAROUND: Rƒôczne obcinanie bez wiedzy o tokenach
const text = sectionContent.text.slice(0, 1500); // Arbitralna warto≈õƒá!

// Rƒôczne ostrze≈ºenie
if (sectionContent.charCount > 1500) {
    console.warn(`‚ö†Ô∏è WARNING: Input truncated from ${sectionContent.charCount} to 1500 chars`);
}
```

**Proponowane API:**
```typescript
const tokenCount = await provider.countTokens(text);
const contextWindow = provider.getContextWindow(); // np. 4096

if (tokenCount > contextWindow - 512) { // -512 na odpowied≈∫
    text = text.slice(0, estimateCharsForTokens(contextWindow - 512));
}
```

**Estymowany nak≈Çad:** 2-3 dni

---

### 2. Wolny inference na CPU/WASM

**Problem:**  
Generowanie 64 token√≥w trwa ~17 sekund na CPU. Dla responsywnych aplikacji to za wolno.

**Uzasadnienie:**  
Nasz cel by≈Ço <10s. Nawet po agresywnej optymalizacji (zmniejszenie modelu, token√≥w, inputu) nie uda≈Ço siƒô zej≈õƒá poni≈ºej 17s.

**Dane benchmarkowe:**
| Konfiguracja | Czas |
|--------------|------|
| Phi-3 (3.8B), 512 token√≥w, 4000 znak√≥w | ~5 minut |
| Qwen 0.5B, 128 token√≥w, 1500 znak√≥w | 31.32s |
| Qwen 0.5B, 64 tokeny, 500 znak√≥w | **17.56s** |

**Jak to obeszli≈õmy:**
```typescript
// Zmniejszenie wszystkiego co mo≈ºliwe
{ maxTokens: 64 }  // zamiast 512
sectionContent.text.slice(0, 500)  // zamiast 4000
model: 'Xenova/Qwen1.5-0.5B-Chat'  // zamiast Phi-3
```

**Propozycja rozwiƒÖzania:**
1. **WebGPU backend** - 10-50x przyspieszenie (priorytet!)
2. **SIMD optimizations** dla WASM
3. **Speculative decoding** dla szybszego generowania

**Estymowany nak≈Çad:** 2-4 tygodnie

---

### 3. Brak `getContextWindow()`

**Problem:**  
Nie ma sposobu na programowe sprawdzenie rozmiaru okna kontekstowego modelu.

**Uzasadnienie:**  
Ka≈ºdy model ma inny limit (Qwen: 4096, Phi-3: 4096, Llama: 8192). Bez tej informacji nie mo≈ºna dynamicznie dostosowaƒá inputu.

**Jak to obeszli≈õmy:**
```typescript
// WORKAROUND: Hardkodowany limit
const MAX_INPUT_CHARS = 1500; // Zgadujemy ≈ºe to bezpieczne
```

**Proponowane API:**
```typescript
const model = await provider.getModelInfo();
console.log(model.contextWindow); // 4096
console.log(model.maxTokens); // 2048
```

**Estymowany nak≈Çad:** 1-2 dni

---

## PRIORYTET WYSOKI üü°

### 4. Brak Abort/Cancel dla inference

**Problem:**  
Nie mo≈ºna przerwaƒá trwajƒÖcego inference. U≈ºytkownik musi czekaƒá nawet je≈õli chce anulowaƒá.

**Uzasadnienie:**  
W aplikacji Stagehand, je≈õli u≈ºytkownik zamknie przeglƒÖdarkƒô w trakcie generowania, proces nadal dzia≈Ça w tle.

**Jak to obeszli≈õmy:**
```typescript
// Brak obej≈õcia - musieli≈õmy czekaƒá lub zabiƒá proces
process.exit(0); // Brutalne rozwiƒÖzanie
```

**Proponowane API:**
```typescript
const controller = new AbortController();
const response = await provider.chat(messages, { 
    signal: controller.signal 
});

// Timeout po 10s
setTimeout(() => controller.abort(), 10000);
```

**Estymowany nak≈Çad:** 3-5 dni

---

### 5. Brak JSON Mode

**Problem:**  
Nie ma gwarancji ≈ºe model zwr√≥ci poprawny JSON. Trzeba parsowaƒá regex i obs≈Çugiwaƒá b≈Çƒôdy.

**Uzasadnienie:**  
Stagehand wymaga strukturalnych odpowiedzi (schematy Zod). Ma≈Çe modele (0.5B) czƒôsto generujƒÖ niepoprawny JSON.

**Jak to obeszli≈õmy:**
```typescript
// WORKAROUND: Regex parsing z fallbackiem
try {
    const jsonMatches = response.content.match(/\{[\s\S]*?\}/g);
    if (jsonMatches) {
        for (const match of jsonMatches) {
            try {
                const parsed = JSON.parse(match);
                if (parsed.talks && parsed.talks.length > 0) {
                    return parsed;
                }
            } catch (e) { /* ignore */ }
        }
    }
} catch (parseError) {
    console.log("‚ö†Ô∏è Could not parse as JSON");
}
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    responseFormat: { 
        type: "json_object",
        schema: z.object({ talks: z.array(...) }) // Zod schema
    }
});
// Gwarantowany poprawny JSON lub error
```

**Estymowany nak≈Çad:** 1 tydzie≈Ñ

---

### 6. Brak Function Calling

**Problem:**  
Nie ma natywnego wsparcia dla tool/function calling, kt√≥re jest standardem w nowoczesnych LLM API.

**Uzasadnienie:**  
Stagehand u≈ºywa function calling do sterowania przeglƒÖdarkƒÖ (click, type, extract). Musieli≈õmy to emulowaƒá w prompcie.

**Jak to obeszli≈õmy:**
```typescript
// WORKAROUND: Instrukcje w prompcie zamiast narzƒôdzi
const prompt = `Extract first event: ${text}
Format: Date - Location - Title`;
// Zamiast:
// tools: [{ name: "extract_event", parameters: {...} }]
```

**Proponowane API:**
```typescript
const response = await provider.chat(messages, {
    tools: [{
        type: "function",
        function: {
            name: "extract_event",
            description: "Extracts event from text",
            parameters: {
                type: "object",
                properties: {
                    date: { type: "string" },
                    location: { type: "string" }
                }
            }
        }
    }]
});

if (response.tool_calls) {
    const call = response.tool_calls[0];
    console.log(call.function.arguments); // { date: "Jan 23", location: "Atlanta" }
}
```

**Estymowany nak≈Çad:** 2 tygodnie

---

## PRIORYTET ≈öREDNI üü¢

### 7. Problemy z Path Aliases w Build

**Problem:**  
Po kompilacji TypeScript, aliasy (`@domain/*`) nie sƒÖ rozwiƒÖzywane, powodujƒÖc b≈Çƒôdy importu.

**Uzasadnienie:**  
Przy pierwszym uruchomieniu przyk≈Çadu Stagehand otrzymali≈õmy b≈ÇƒÖd:
```
Cannot find module '@domain/errors'
```

**Jak to naprawili≈õmy:**
```json
// package.json - musieli≈õmy dodaƒá tsc-alias
"scripts": {
    "build": "tsc && tsc-alias"  // Dodatkowy krok!
}
```

**Propozycja:**
- Rozwa≈ºyƒá u≈ºycie `tsup` lub `esbuild` zamiast raw `tsc`
- Lub do≈ÇƒÖczyƒá `tsc-alias` jako dependency i zautomatyzowaƒá

**Estymowany nak≈Çad:** 0.5 dnia

---

### 8. Konflikt wersji ONNX Runtime

**Problem:**  
LXRT wymaga `onnxruntime-node@1.23.0`, ale `@huggingface/transformers` wymaga `1.21.0`.

**Uzasadnienie:**  
Przy instalacji otrzymali≈õmy ostrze≈ºenia o konflikcie, a p√≥≈∫niej b≈Çƒôdy ≈Çadowania modelu:
```
Protobuf parsing failed
```

**Jak to naprawili≈õmy:**
```bash
# Usuniƒôcie nadmiarowej zale≈ºno≈õci
npm uninstall onnxruntime-node
# U≈ºycie wersji z @huggingface/transformers
```

**Propozycja:**
- UsunƒÖƒá bezpo≈õredniƒÖ zale≈ºno≈õƒá `onnxruntime-node` z package.json
- Polegaƒá na wersji dostarczanej przez `@huggingface/transformers`

**Estymowany nak≈Çad:** 0.5 dnia

---

### 9. Brak typ√≥w dla Event Payloads

**Problem:**  
Eventy `progress` i `ready` nie majƒÖ wyeksportowanych typ√≥w TypeScript.

**Uzasadnienie:**  
IDE nie podpowiada dostƒôpnych p√≥l, co utrudnia development.

**Jak to obeszli≈õmy:**
```typescript
// Zgadywanie struktury na podstawie log√≥w
provider.on('progress', (data) => {
    const percent = data.progress?.toFixed(1) ?? '?';  // Nie wiemy czy istnieje!
    const file = data.file ?? 'unknown';
    const status = data.status ?? 'loading';
    // ...
});
```

**Proponowane typy:**
```typescript
// Eksportowaƒá z biblioteki:
export interface ProgressEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
    file: string;
    progress: number; // 0-100
    loaded: number;   // bytes
    total: number;    // bytes
    status: 'downloading' | 'loading' | 'ready';
}

export interface ReadyEvent {
    modality: 'llm' | 'embeddings' | 'vision';
    model: string;
}
```

**Estymowany nak≈Çad:** 0.5 dnia

---

### 10. Dokumentacja Streaming API

**Problem:**  
Brak dokumentacji jak u≈ºywaƒá `provider.stream()`.

**Uzasadnienie:**  
Musieli≈õmy szukaƒá w kodzie ≈∫r√≥d≈Çowym (`grep_search stream`) aby znale≈∫ƒá ≈ºe ta funkcja istnieje.

**Jak to odkryli≈õmy:**
```bash
# Szukanie w ≈∫r√≥d≈Çach
grep -r "stream" src/app/AIProvider.ts
# Znale≈∫li≈õmy: async *stream(
```

**Propozycja:**
Dodaƒá do dokumentacji:
```markdown
## Streaming Responses

```typescript
for await (const token of provider.stream(messages, options)) {
    process.stdout.write(token);
}
```
```

**Estymowany nak≈Çad:** 0.5 dnia

---

### 11. Brak przyk≈Çad√≥w integracji

**Problem:**  
Brak oficjalnych przyk≈Çad√≥w integracji z popularnymi frameworkami.

**Uzasadnienie:**  
Musieli≈õmy od zera pisaƒá `LxrtLLMProvider` dla Stagehand, zgadujƒÖc jak mapowaƒá API.

**Jak to zrobili≈õmy:**
```typescript
// Napisali≈õmy w≈Çasny adapter (109 linii kodu)
export class LxrtLLMProvider implements LLMClient {
    async createChatCompletion(options) {
        // Mapowanie Stagehand -> LXRT
        const messages = options.messages.map(m => ({
            role: m.role,
            content: typeof m.content === 'string' ? m.content : '...'
        }));
        // ...
    }
}
```

**Propozycja:**
Stworzyƒá oficjalne adaptery:
- `@lxrt/stagehand` - adapter dla Stagehand
- `@lxrt/langchain` - adapter dla LangChain.js
- `@lxrt/vercel-ai` - adapter dla Vercel AI SDK

**Estymowany nak≈Çad:** 1 tydzie≈Ñ per adapter

---

### 12. Brak CLI do zarzƒÖdzania modelami

**Problem:**  
Nie ma sposobu na pre-download modeli przed uruchomieniem aplikacji.

**Uzasadnienie:**  
Pierwszy start aplikacji trwa d≈Çugo (pobieranie modelu). W produkcji chcemy mieƒá modele ju≈º pobrane.

**Jak to obeszli≈õmy:**
```typescript
// Model pobiera siƒô przy pierwszym warmup()
await provider.warmup('llm'); // Tutaj dopiero ≈õciƒÖga ~1GB
```

**Proponowane CLI:**
```bash
# Pre-download modeli
npx lxrt pull Xenova/Qwen1.5-0.5B-Chat --dtype q4

# Lista pobranych modeli
npx lxrt list

# Usu≈Ñ model z cache
npx lxrt remove Xenova/Phi-3-mini-4k-instruct
```

**Estymowany nak≈Çad:** 1 tydzie≈Ñ

---

### 13. Brak Registry i Type-Safety dla modeli

**Problem:**  
Wszystkie konfiguracje modeli (`LLMConfig`, `STTConfig`) u≈ºywajƒÖ typu `string` dla pola `model`. Brak weryfikacji czy model istnieje oraz brak autouzupe≈Çniania w IDE.

**Uzasadnienie:**  
Programista musi znaƒá dok≈Çadne ID modelu z Hugging Face (np. `Xenova/whisper-tiny`). Liter√≥wka powoduje b≈ÇƒÖd dopiero w runtime (przy pr√≥bie pobrania).

**Jak to obeszli≈õmy:**  
Rƒôczne wpisywanie string√≥w bez walidacji.

**Proponowane rozwiƒÖzanie (Model Registry):**
Implementacja podej≈õcia "Registry + Type-Safety":
- **Registry:** Centralny plik `src/core/ModelRegistry.ts` z definicjami przetestowanych modeli.
- **Typy:** `type SupportedLLM = keyof typeof MODEL_REGISTRY.llm`.
- **Hybrid types:** `model: SupportedLLM | (string & {})` - zapewnia autouzupe≈Çnianie dla znanych modeli, zachowujƒÖc mo≈ºliwo≈õƒá wpisania dowolnego stringa.

**Estymowany nak≈Çad:** 2-3 dni

---

### 14. Robust Integration Testing (Prawdziwe modele + Determinizm)

**Problem:**
Testy integracyjne (np. STT -> LLM) sƒÖ "flaky" (niestabilne) z powodu niedoskona≈Ço≈õci ma≈Çych modeli (Whisper Tiny) na syntetycznych danych lub szumie. Workaroundy (jak `if text == '!!!'`) sƒÖ tymczasowe.

**RozwiƒÖzanie (Jak):**
1.  **Golden Datasets:** Stworzenie repozytorium prawdziwych pr√≥bek audio (human voice, clear speech) zamiast generowanych/pustych.
2.  **Semantic Assertions:** Weryfikacja poprawno≈õci nie przez `text.length > 0`, ale przez podobie≈Ñstwo semantyczne (np. czy odpowied≈∫ LLM ma sens w kontek≈õcie).
3.  **Determinizm:** Ustawienie `seed` dla modeli (je≈õli wspierane) oraz `temperature=0` w testach.

**Estymowany nak≈Çad:** 2-3 dni

---

## Podsumowanie Priorytet√≥w

| # | Zadanie | Priorytet | Nak≈Çad | Wp≈Çyw |
|---|---------|-----------|--------|-------|
| 1 | `countTokens()` | üî¥ Krytyczny | ‚úÖ DONE | Wysoki |
| 2 | WebGPU backend | üî¥ Krytyczny | 2-4 tyg | Bardzo wysoki |
| 3 | `getContextWindow()` | üî¥ Krytyczny | ‚úÖ DONE | Wysoki |
| 4 | Abort/Cancel | üü° Wysoki | 3-5 dni | ≈öredni |
| 5 | JSON Mode | üü° Wysoki | 1 tydzie≈Ñ | Wysoki |
| 6 | Function Calling | üü° Wysoki | 2 tygodnie | Wysoki |
| 7 | Fix path aliases | üü¢ ≈öredni | ‚úÖ DONE | Niski |
| 8 | Fix ONNX conflict | üü¢ ≈öredni | ‚úÖ DONE | Niski |
| 9 | Typy event√≥w | üü¢ ≈öredni | ‚úÖ DONE | Niski |
| 10 | Docs streaming | üü¢ ≈öredni | ‚úÖ DONE | Niski |
| 11 | Adaptery integracji | üü¢ ≈öredni | 3 tygodnie | ≈öredni |
| 12 | CLI zarzƒÖdzania | üü¢ ≈öredni | 1 tydzie≈Ñ | ≈öredni |
| 13 | Model Registry & Types | üü¢ ≈öredni | ‚úÖ DONE | ≈öredni |
| 14 | **Robust Integration Testing** | üü¢ ≈öredni | ‚úÖ DONE | ≈öredni |
| 15 | Refactor `Error` to `ModelNotLoadedError` | üü¢ Niski | ‚úÖ DONE | Niski |
| 16 | Unify error strings (constants) | üü¢ Niski | ‚úÖ DONE | Niski |
| 17 | **Test Quality Review & Rewrite** | üü¢ ≈öredni | 2-3 dni | ≈öredni |
| 18 | **Stagehand Interface** | üü¢ ≈öredni | 2 dni | Wysoki |

**Sugerowana kolejno≈õƒá na nastƒôpny cykl:**
1. Fix ONNX conflict + path aliases (szybkie wygrane)
2. `countTokens()` + `getContextWindow()` (krytyczne dla UX)
3. **Robust Integration Testing** (blokuje CI/CD)
4. **Stagehand Interface** (Wa≈ºne dla integracji)
5. **Test Quality Review** (D≈Çug techniczny)
6. Abort/Cancel + typy event√≥w
5. Dokumentacja streaming + przyk≈Çady
6. WebGPU (d≈Çugoterminowy, ale game-changer)

---

## Przysz≈Çe Rozszerzenia - Auto-Tuning System

### Implementacja w Fazach (patrz: autotuning_plan.md)

#### Faza 0: Model Presets ‚úÖ ZAKO≈ÉCZONE
**Status:** Implementacja statycznych preset√≥w (`chat-light`, `embedding-quality`)  
**Cel:** Foundation dla auto-tuningu - semantic naming dla modeli  
**Nak≈Çad:** 1-2 dni

#### Faza 1-5: Auto-Tuning (Priorytetowe)

| # | Funkcja | Priorytet | Nak≈Çad | Opis |
|---|---------|-----------|--------|------|
| 1 | **Model Selection** | üî¥ Bardzo wysoki | ‚úÖ ZAKO≈ÉCZONE | Auto-wyb√≥r modelu na podstawie RAM, GPU, platform |
| 2 | **DType Selection** | üî¥ Wysoki | ‚úÖ ZAKO≈ÉCZONE | Auto kwantyzacja (fp16/q8/q4) na podstawie zasob√≥w |
| 3 | **Performance Mode** | üü° ≈öredni | ‚úÖ ZAKO≈ÉCZONE | Auto fast/balanced/quality w zale≈ºno≈õci od ≈õrodowiska |
| 4 | **WASM Threads** | ‚úÖ Ju≈º dzia≈Ça | ‚úÖ ZAKO≈ÉCZONE | Ulepszenia istniejƒÖcej logiki thread count |
| 5 | **Context/Tokens Limits** | üü¢ Niski | ‚úÖ ZAKO≈ÉCZONE | Auto-limitowanie dla s≈Çabych system√≥w (OOM prevention) |

**Total Faza 1-5:** ~8-12 dni roboczych


### Specyfikacja Refaktoryzacji (Zadania #15, #16)

#### Zadanie #15: Refactor `Error` to `ModelNotLoadedError`

**Cel:** Umo≈ºliwienie programistycznej obs≈Çugi b≈Çƒôd√≥w (np. auto-warmup po z≈Çapaniu b≈Çƒôdu).

**Wymagania:**
1.  Stworzyƒá klasƒô `ModelNotLoadedError` w `src/domain/errors.ts`:
    ```typescript
    export class ModelNotLoadedError extends BaseError {
      constructor(
        message: string,
        public modality: Modality,
        public modelId?: string
      ) {
        super(message);
        this.name = 'ModelNotLoadedError';
      }
    }
    ```
2.  **Use Cases (Gdzie u≈ºyƒá):**
    *   `AIProvider.countTokens()` - gdy config istnieje, ale model nie loaded.
    *   `AIProvider.getContextWindow()`
    *   `AIProvider.chat()`, `speak()`, `listen()` - zastƒÖpiƒá obecne `ValidationError` lub generic `Error` tam, gdzie sprawdzany jest stan za≈Çadowania.

#### Zadanie #16: Unify Error Strings

**Cel:** Unikniƒôcie liter√≥wek i niesp√≥jnych komunikat√≥w ("Model not loaded" vs "Load model first").

**Wymagania:**
1.  Utworzyƒá `src/core/error-messages.ts`:
    ```typescript
    export const ERRORS = {
      MODEL: {
        NOT_LOADED: (modality: string) => 
          `Model for ${modality} must be loaded. Call warmup('${modality}') first.`,
        NOT_CONFIGURED: (modality: string) =>
          `${modality} not configured. Add config to createAIProvider().`,
      },
      // ...
    } as const;
    ```
2.  ZastƒÖpiƒá hardcoded stringi w `AIProvider.ts` i `LLMModel.ts`.

---

#### Przysz≈Çe Ulepszenia (P√≥≈∫niej)

| # | Funkcja | Priorytet | Nak≈Çad | Opis |
|---|---------|-----------|--------|------|
| 6 | **Batch Size Tuning** | üü° ≈öredni | 2-3 dni | Automatyczny batch size dla embeddings na podstawie RAM/GPU |
| 7 | **Cache Strategy** | üü¢ Niski | 3-5 dni | Inteligentne zarzƒÖdzanie cache (eviction, quota management) |
| 8 | **Inference Params** | üü¢ Niski | 1-2 dni | Auto-tuning temperature, topK, topP dla r√≥≈ºnych use-cases |

**Przyk≈Çadowe API po auto-tuningu:**
```typescript
const provider = createAIProvider({
  llm: {
    preset: 'chat',      // intencja u≈ºytkownika
    autoTune: true       // auto: model + dtype + performance
  }
});

// System automatycznie wybiera:
// - Model: chat-light/medium/heavy na podstawie RAM & GPU  
// - DType: fp16/q8/q4 na podstawie capabilities
// - Performance: fast/balanced/quality
// - Threads: optimal count
// - MaxTokens: safe limits
```

**Wiƒôcej:** Szczeg√≥≈Çy implementacji w `autotuning_plan.md` (artifact)

---

## Za≈ÇƒÖczniki

### A. Kod adaptera Stagehand
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/LxrtLLMProvider.ts`

### B. Zoptymalizowany przyk≈Çad
Lokalizacja: `/home/pyroxar/Pulpit/lxrt/examples/stagehand/src/index.ts`

### C. Benchmark wynik√≥w
- Model: Qwen1.5-0.5B-Chat (q4)
- Czas: 17.56s dla 64 token√≥w
- Platform: CPU/WASM (Linux x64)
