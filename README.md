<div align="center">

# LXRT â€” Lokalna Infrastruktura AI

`transformers.js` Â· `llm` Â· `tts/stt` Â· `embeddings` Â· `ocr` Â· `vectorization` Â· `web-workers`

[![npm version](https://img.shields.io/npm/v/lxrt.svg)](https://www.npmjs.com/package/lxrt)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)

**Biblioteka TypeScript do uruchamiania modeli AI w jednolitej infrastrukturze**

</div>

---

## Dlaczego LXRT?

| Cecha | Opis |
|-------|------|
| ğŸ”’ **Local-first** | PrywatnoÅ›Ä‡, niÅ¼sze koszty, niÅ¼sze opÃ³Åºnienia |
| ğŸ¯ **Jedna warstwa infrastruktury** | Modele, cache, workery, backend (WebGPU/WASM/Node), progress, vector store |
| ğŸ”Œ **Gotowe integracje** | OpenAI-compatible, LangChain, React/Vue hooks |
| ğŸ­ **Multi-modalne** | LLM, TTS, STT, Embeddings, OCR, audio/video/image vectorization dla RAG |
| ğŸš€ **WebGPU Ready** | Automatyczna akceleracja sprzÄ™towa (10-50x faster) w przeglÄ…darce |
| ğŸ“ **TypeScript-first** | PeÅ‚ne typowanie, czyste API |

---

## Instalacja

```bash
npm install lxrt @huggingface/transformers
# lub yarn / pnpm
```

## CLI (ZarzÄ…dzanie Modelami)

LXRT udostÄ™pnia narzÄ™dzie wiersza poleceÅ„ do zarzÄ…dzania lokalnymi modelami:

```bash
# Pobranie modelu z Hugging Face Hub (z paskiem postÄ™pu)
npx lxrt pull Xenova/Qwen1.5-0.5B-Chat --dtype q4

# Lista pobranych modeli
npx lxrt list

# UsuniÄ™cie modelu
npx lxrt remove Xenova/Qwen1.5-0.5B-Chat
```

---

## Szybki Start

### Podstawowy chat z LLM

```typescript
import { createAIProvider } from 'lxrt';

const provider = createAIProvider({
  llm: {
    model: 'onnx-community/Qwen2.5-0.5B-Instruct',
    dtype: 'q4',
  },
  backend: { prefer: 'webgpu', fallback: 'wasm' },
});

const reply = await provider.chat([
  { role: 'user', content: 'CzeÅ›Ä‡, LXRT!' }
]);
console.log(reply.content);
```

### Multi-modalna konfiguracja

```typescript
const provider = createAIProvider({
  llm: { model: 'onnx-community/Qwen2.5-0.5B-Instruct', dtype: 'q4' },
  tts: { model: 'Xenova/speecht5_tts', dtype: 'fp32' },
  stt: { model: 'Xenova/whisper-tiny', dtype: 'fp32', language: 'pl' },
  embedding: { model: 'Xenova/all-MiniLM-L6-v2', dtype: 'fp32' },
  ocr: { language: ['pol', 'eng'] },
});
```

---

## Kluczowe MoÅ¼liwoÅ›ci

### ğŸ’¬ LLM / Chat

```typescript
// Chat z historiÄ…
const response = await provider.chat([
  { role: 'system', content: 'JesteÅ› pomocnym asystentem.' },
  { role: 'user', content: 'Czym jest TypeScript?' },
]);

// Streaming
for await (const token of provider.stream('Opowiedz historiÄ™')) {
  process.stdout.write(token);
}

// JSON Mode
const jsonRel = await provider.chat('WymieÅ„ 3 miasta w JSON', {
  responseFormat: { type: 'json_object' }
});

// Function Calling
const tools = [{
  type: 'function',
  function: {
    name: 'get_weather',
    parameters: { type: 'object', properties: { location: { type: 'string' } } }
  }
}];
const toolRes = await provider.chat('Jaka pogoda w Warszawie?', { tools });
console.log(toolRes.toolCalls); // [{ name: 'get_weather', arguments: '{"location":"Warszawa"}' }]

```

### ğŸ”Š TTS (Text-to-Speech)

```typescript
const audio = await provider.speak('Witaj Å›wiecie!', {
  voiceProfile: 'professional-female',
});
```

### ğŸ¤ STT (Speech-to-Text)

```typescript
const text = await provider.listen(audioBlob, {
  language: 'pl',
});
```

### ğŸ“¸ OCR

```typescript
const result = await provider.recognize(imageFile, {
  language: ['pol', 'eng'],
  autoLanguage: true,
});
console.log(result.text);
```

### ğŸ§® Embeddingi i Wyszukiwanie Semantyczne

```typescript
// Embeddingi
const vectors = await provider.embed(['tekst 1', 'tekst 2']);

// PodobieÅ„stwo
const score = await provider.similarity('Kocham programowanie', 'Uwielbiam kodowaÄ‡');

// Wyszukiwanie
const result = await provider.findSimilar('Kot na macie', dokumenty);
```

### ğŸ“Š Wektoryzacja (RAG)

Wsparcie dla plikÃ³w tekstowych, **PDF** oraz **DOCX**:

```typescript
await provider.initializeVectorization({ storage: 'indexeddb' });

// Automatyczna ekstrakcja tekstu z PDF/DOCX
await provider.indexFiles([
    new File([pdfBlob], "dokument.pdf", { type: "application/pdf" }),
    new File([docxBlob], "pismo.docx", { type: "application/vnd.openxmlformats-officedocument.wordprocessingml.document" })
]);

const results = await provider.queryVectors('Jak dziaÅ‚a AI?');
```

### ğŸ¯ Model Registry i Type Safety

LXRT zapewnia **type-safe model registry** z auto-completion dla wspieranych modeli:

```typescript
import { createAIProvider, type SupportedLLM, MODEL_REGISTRY, getModelInfo } from 'lxrt';

// âœ… Auto-completion dla znanych modeli
const model: SupportedLLM = 'Xenova/Qwen1.5-0.5B-Chat';

// âœ… Nadal moÅ¼na uÅ¼ywaÄ‡ dowolnych stringÃ³w
const customModel = 'my-org/my-custom-model';

// Pobranie informacji o modelu
const info = getModelInfo('llm', 'Xenova/Qwen1.5-0.5B-Chat');
console.log(info?.contextWindow); // 32768
console.log(info?.family); // 'qwen'

// PrzeglÄ…danie wszystkich modeli
console.log(MODEL_REGISTRY.llm);
console.log(MODEL_REGISTRY.embedding);
```

### ğŸ·ï¸ Model Presets (Semantic Naming)

LXRT oferuje **presety** - semantyczne nazwy dla modeli, uÅ‚atwiajÄ…ce wybÃ³r odpowiedniego rozwiÄ…zania bez znania konkretnych ID.

```typescript
const provider = createAIProvider({
  // Zamiast 'Xenova/Qwen1.5-0.5B-Chat'
  llm: { model: 'chat-light' },
  
  // Zamiast 'Xenova/all-MiniLM-L6-v2'
  embedding: { model: 'embedding-quality' },
  
  // DziaÅ‚a teÅ¼ 'fast', 'balanced', 'quality'
  stt: { model: 'fast' }
});
```

**DostÄ™pne presety (LLM):**
- `tiny` (<1GB, GPT-2)
- `chat-light` (~2GB, Qwen 1.5 0.5B)
- `chat-medium` (~4GB, Phi-3 Mini)
- `chat-heavy` (>4GB, Gemma 2B)
- `fast` / `balanced` / `quality`

### ğŸ›ï¸ Auto-Tuning (Inteligentny WybÃ³r Modelu)

LXRT potrafi **automatycznie dobraÄ‡ najlepszy model** na podstawie Twojego sprzÄ™tu (RAM, GPU). Wystarczy dodaÄ‡ flagÄ™ `autoTune: true`:

```typescript
const provider = createAIProvider({
  llm: { 
    model: 'chat', // ogÃ³lna intencja
    autoTune: true // pozwÃ³l na automatyczny dobÃ³r
  }
});

// Wynik autotuningu:
// - High-end PC (32GB RAM + GPU) -> 'chat-heavy' (Gemma 2B)
// - Laptop (8GB RAM) -> 'chat-medium' (Phi-3 Mini)
// - SÅ‚aby sprzÄ™t / Browser -> 'chat-light' (Qwen 0.5B)
```

### ğŸ”¢ Liczenie TokenÃ³w i Context Window

```typescript
const provider = createAIProvider({
  llm: { model: 'Xenova/Qwen1.5-0.5B-Chat' }
});

await provider.warmup('llm');

// SprawdÅº rozmiar okna kontekstowego
const contextWindow = provider.getContextWindow(); // 32768

// Policz tokeny w tekÅ›cie
const text = 'To jest przykÅ‚adowy tekst do analizy.';
const tokenCount = provider.countTokens(text); // ~12

// Upewnij siÄ™ Å¼e tekst mieÅ›ci siÄ™ w oknie
if (tokenCount > contextWindow - 512) {
  // Obetnij tekst aby zmieÅ›ciÅ‚ siÄ™ w limicie
  console.warn('Tekst za dÅ‚ugi, obcinanie...');
}
```

---

## Adaptery

### OpenAI-compatible

```typescript
import { OpenAIAdapter } from 'lxrt';

const client = new OpenAIAdapter(provider);
const resp = await client.chat.completions.create({
  model: 'local-llm',
  messages: [{ role: 'user', content: 'CzeÅ›Ä‡!' }],
});
```

### âœ… Vercel AI SDK

Adapter umoÅ¼liwia uÅ¼ycie LXRT jako dostawcy w Vercel AI SDK (streaming response):

```typescript
import { createVercelProvider } from 'lxrt/adapters';
import { streamText } from 'ai';

const provider = createVercelProvider(lxrtProvider);
const result = await streamText({
  model: provider.languageModel('local-model'),
  prompt: 'Dlaczego niebo jest niebieskie?',
});
```

### ğŸ­ Stagehand (Browser Automation)

UÅ¼yj LXRT do sterowania przeglÄ…darkÄ… w Stagehand:

```typescript
import { StagehandAdapter } from 'lxrt/adapters';

const model = new StagehandAdapter(provider, 'Xenova/Qwen1.5-0.5B-Chat');
// UÅ¼yj 'model' w konfiguracji Stagehand
```

### ğŸ¦œğŸ”— LangChain

```typescript
import { createLangChainLLM } from 'lxrt/adapters';

const llm = createLangChainLLM(provider);
const res = await llm.invoke('Opowiedz Å¼art o kotach');
```

---

## React / Vue

### React Hook

```tsx
import { useChat } from 'lxrt/react';

function Chat() {
  const { messages, sendMessage, isLoading } = useChat();
  
  return (
    <div>
      {messages.map((m, i) => <div key={i}>{m.content}</div>)}
      <button onClick={() => sendMessage('CzeÅ›Ä‡!')}>WyÅ›lij</button>
    </div>
  );
}
```

### Vue Composable

```typescript
import { useChat } from 'lxrt/vue';

const { messages, sendMessage, isLoading } = useChat();
await sendMessage('CzeÅ›Ä‡!');
```

---

## PostÄ™p Åadowania

```typescript
provider.on('progress', ({ modality, file, progress }) => {
  console.log(`Åadowanie ${modality}: ${file} (${progress}%)`);
});

provider.on('ready', ({ modality }) => {
  console.log(`âœ“ ${modality} gotowy`);
});
```

---

## Konfiguracja

| Opcja | WartoÅ›ci | Opis |
|-------|----------|------|
| **Backend** | `webgpu` / `wasm` / `node` | Auto-fallback |
| **DType** | `fp32` / `fp16` / `q8` / `q4` / `q4f16` | Precyzja modelu |
| **Cache** | Automatyczny | Przechowywanie i ponowne uÅ¼ycie modeli |
| **Workers** | Web Workers | CiÄ™Å¼kie obliczenia poza main thread |
| **Vector Store** | IndexedDB | Lokalne przechowywanie dla RAG |

---

## Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIProvider                        â”‚
â”‚  (fasada gÅ‚Ã³wna - chat/speak/listen/embed/ocr)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Models     â”‚   Services   â”‚    Infrastructure     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLMModel     â”‚ ModelManager â”‚ BackendSelector       â”‚
â”‚ TTSModel     â”‚ ModelCache   â”‚ WorkerPool            â”‚
â”‚ STTModel     â”‚ Vectorize-   â”‚ VectorStore (IDB)     â”‚
â”‚ EmbeddingM.  â”‚ tionService  â”‚ EventEmitter          â”‚
â”‚ OCRModel     â”‚              â”‚                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                              â”‚
        â–¼                              â–¼
   Transformers.js              Tesseract.js
```

SzczegÃ³Å‚owy opis: [ARCHITECTURE.md](./ARCHITECTURE.md)

---

## Dokumentacja

| Dokument | Opis |
|----------|------|
| [API.md](./API.md) | PeÅ‚na dokumentacja API |
| [ARCHITECTURE.md](./ARCHITECTURE.md) | Architektura i relacje miÄ™dzy komponentami |
| [WEBGPU_GUIDE.md](./docs/WEBGPU_GUIDE.md) | Przewodnik po akceleracji WebGPU |
| [EXAMPLES.md](./EXAMPLES.md) | PrzykÅ‚ady uÅ¼ycia |

---

## PrzykÅ‚ady

Katalog [`examples/`](./examples/) zawiera:

- `basic.js` - Podstawowe uÅ¼ycie
- `multimodal.js` - LLM + TTS + STT + Embeddings
- `agent-integration.js` - Integracja z agentami AI
- `ocr-basic.js` - Rozpoznawanie tekstu OCR
- `tts-voice-profiles.js` - Profile gÅ‚osowe TTS
- `react-chat-example.tsx` - Hook React
- `vue-chat-example.vue` - Composable Vue
- `worker-chat.html` - Web Workers

---

## Wymagania

- Node.js >= 24.13.0
- PrzeglÄ…darka z WebGPU (opcjonalnie, fallback do WASM)

### Peer Dependencies

```json
{
  "@huggingface/transformers": "^3.0.0"
}
```

---

## Licencja

MIT Â© [Kacper Paczos](https://github.com/kacperpaczos)

---

## Linki

- [GitHub](https://github.com/kacperpaczos/lxrt)
- [npm](https://www.npmjs.com/package/lxrt)
- [ZgÅ‚oÅ› problem](https://github.com/kacperpaczos/lxrt/issues)
